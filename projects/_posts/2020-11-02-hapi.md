---
layout: post
title: >
    HAPI: Hardware-Aware Progressive Inference
description: >
  Convolutional neural networks (CNNs) have recently become the state-of-the-art in a diversity of AI tasks. Despite their popularity, CNN inference still comes at a high computational cost. A growing body of work aims to alleviate this by exploiting the difference in the classification difficulty among samples and early-exiting at different stages of the network. Nevertheless, existing studies on early exiting have primarily focused on the training scheme, without considering the use-case requirements or the deployment platform. This work presents HAPI, a novel methodology for generating high-performance early-exit networks by co-optimising the placement of intermediate exits together with the early-exit strategy at inference time. Furthermore, we propose an efficient design space exploration algorithm which enables the faster traversal of a large number of alternative architectures and generates the highest-performing design, tailored to the use-case requirements and target hardware. Quantitative evaluation shows that our system consistently outperforms alternative search mechanisms and state-of-the-art early-exit schemes across various latency budgets. Moreover, it pushes further the performance of highly optimised hand-crafted early-exit CNNs, delivering up to 5.11Ã— speedup over lightweight models on imposed latency-driven SLAs for embedded devices.
accent_image: 
  background: url('/assets/img/blog/research.jpg') top/cover
  overlay: true
invert_sidebar: false
sitemap: false
hide_last_modified: true
---

**Authors:** **S. Laskaridis**\*, S. I. Venieris\*, H. Kim, N. D. Lane

**Published at:** _International Conference on Computer-Aided Design (ICCAD'20)_


## Overview

![hapi](/assets/img/blog/hapi/hapi-overview.png)
<!-- ![hapi-inference-engine](/assets/img/blog/hapi/hapi-inference-engine.png) -->

## Links

* [paper](https://dl.acm.org/doi/abs/10.1145/3400302.3415698)
* [preprint](https://arxiv.org/abs/2008.03997)

