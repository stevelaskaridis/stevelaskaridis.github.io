---
layout: post
title: >
    Maestro: Uncovering Low-Rank Structures via Trainable Decomposition
description: >
  Deep Neural Networks (DNNs) have been a large driver for AI breakthroughs in recent years, ranging from self-driving cars to intelligent assistants. However, these models have been getting increasingly large as they become more accurate and safe. This means that their training becomes increasingly costly and time-consuming, and typically yields a single model to fit all targets. To mitigate this, various techniques have been proposed in the literature, including pruning, sparsification or quantization of the model weights and updates. While achieving high compression rates, they often incur significant computational overheads at training or lead to non-negligible accuracy penalty. Alternatively, factorization methods have been leveraged for low-rank compression of DNNs. Similarly, such techniques (e.g., SVD) frequently rely on heavy iterative decompositions of layers and are potentially sub-optimal for non-linear models, such as DNNs. We take a further step in designing efficient low-rank models and propose Maestro, a framework for trainable low-rank layers. Instead of iteratively applying a priori decompositions, the low-rank structure is baked into the training process through LoD, a low-rank ordered decomposition. Not only is this the first time importance ordering via sampling is applied on the decomposed DNN structure, but it also allows selecting ranks at a layer granularity. Our theoretical analysis demonstrates that LoD recovers the SVD decomposition of linear mapping on uniformly distributed data and PCA for linear autoencoders. Applied to DNNs, Maestro enables the extraction of lower footprint models that preserve performance. Simultaneously, it enables the graceful tradeoff between accuracy-latency for deployment to even more constrained devices, without retraining.
accent_image:
  background: url('/assets/img/blog/research.jpg') top/cover
  overlay: true
invert_sidebar: false
sitemap: false
hide_last_modified: true
---

**Authors:** Samuel Horvath, **Stefanos Laskaridis**, Shashank Rajput, Hongyi Wang

**Published at:** _International Conference on Machine Learning (ICML'24)_ and _Workshop on Advancing Neural Network Training (WANT-AI @ NeurIPS’23)_

## Overview

![Maestro](/assets/img/blog/maestro/MaestroPipeline.png)
![Poster](/assets/img/blog/maestro/maestro_poster.png)

With Maestro, we asked whether low-rank structure could be learned during training rather than imposed afterward through expensive decompositions. We proposed LoD (low-rank ordered decomposition), a trainable factorization that allows each layer to learn its effective rank as part of optimization.

This approach lets us produce models that are compact by construction and support graceful accuracy–latency trade-offs at inference time, without retraining. We also provide theoretical insights connecting LoD to SVD and PCA in special cases. In practice, Maestro enables a “train once, deploy everywhere” workflow across diverse deployment constraints.

## Links

* [paper](https://openreview.net/forum?id=7bjyambg4x)
* [preprint](https://arxiv.org/abs/2308.14929)
* [talk @ MBZUAI](https://www.youtube.com/watch?v=aYkCAIEtEFM)
* [code](https://github.com/stevelaskaridis/maestro-lod)